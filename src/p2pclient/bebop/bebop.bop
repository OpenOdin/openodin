
/**
 * This struct is part of Match and is used to limit how many times a Match matches on specific
 * fields.
 *
 * When processing a query having a match with limitField set then for each identical value of the
 * field  identified by limitField.name a counter is increased and when the counter is greater then
 * limitField.limit the following nodes who match limitField are ignored and not part of the
 * result set.
 *
 * A match state is reset for each query below a node, meaning a limitField constraint is local to a
 * set of sibling nodes.
 *
 * The usage of this could be to limit the number of occurrences of nodes returned who have the field
 * "name" set to the same value.
 *
 * For example setting limitField to `{name: "Highlander", limit: 1}` will only allow one node having
 * the field `name` set to "Highlander" within the same set of sibling nodes.
 *
 * Note that if the chosen node is later discarded due to its timestamp the matched node will not be
 * returned in the result set as will no other node who was constrained under the limitField.
 * In these cases one could increase the `limit` or try reversing (descending=true) the result set
 * to pick from neweset nodes instead of oldest nodes first.
 */
struct LimitField {
    /** Name of the field to count identical values on. */
    string name;

    /** Limit of identical hits on the field to return results for. */
    uint16 limit;
}

/** This struct matched Filter in datamodel/model/types.ts. */
struct Filter {
    /**
     * The name of the field which value to compare.
     *
     * Special cases:
     * - If "id" then gets id2, if no id2 then gets the id1 value.
     */
    string field;

    /**
     * An optional operator which can be applied to some data field types.
     * On unsigned integer fields UINT8, UINT16, UINT24, UINT32 the following
     * bitwise operations can be applied:
     *  & x     AND a bitmask with an integer field,
     *  | x     OR a bitmask with an integer field,
     *  ^ x     XOR a bitmask with an integer field,
     *  >> x    right shift an integer field by x,
     *  << x    left shift an integer field by x.
     *
     *  On string or bytes fields the slice operator can be applied, as:
     *  :1,3   cut a string or bytes field by index:length where ",length" is optional,
     *  :-1    cut a string or bytes field by index:length where start index counts from end.
     *
     *  On string or bytes fields the hash operator can be applied, as:
     *  hash   perform a blake2b 32 byte hashing on the field data before comparison.
     *         string are utf8 encoded into buffer before hashing.
     */
    string operator;

    /** The comparison operator to use. */
    string cmp;

    /**
     * The value to compare the field value to.
     *
     * The resulting type is determined by the type of the field comparing to.
     *
     * For number, bigint the value is parsed from string to number/bigint.
     *
     * For Buffer string is hexadecimal encoded into Buffer.
     */
    string value;
}

/**
 * A FetchQuery must have atleast one Match struct to match on nodes.
 * A Match is processed for each set of nodes in the tree in respect to their level in the tree.
 * Matches must have matched on levels above to be able to traverse downwards on those matched nodes.
 */
struct Match {
    /** Up to six bytes, lesser means wildcard match on node type. */
    byte[] nodeType;

    /**
     * For a Match to be valid all Filters (if any given) have to match.
     *
     * For filter.field === "creationTime" and filter.value < 0 then value is automatically
     * recalculated as Date.now() + value; This can be used to have a sliding window.
     */
    Filter[] filters;

    /**
     * Maximum nodes allowed for this match.
     * -1 means no limit.
     */
    int32 limit;

    /** Limit max nr of key:value occurrences for this filter so it does not exceed limit. */
    LimitField limitField;

    /**
     * The levels in the tree this Match is applicable to.
     * Leave empty for all levels.
     * 0 is root level of the FetchQuery, however Matches do not match at level 0
     * since the root node is already matched on its id in the query.
     * The match state is reset for each new level for Match's used on multiple levels.
     */
    uint16[] level;

    /**
     * If set then the node will be discarded from the resultset.
     * But up til that last moment the node will be used for traversing just as if not discarded,
     * and also match state is managed as when not discarding.
     * It is enough that a single Match object has discard=false for the node to be kept in the
     * result set.
     */
    bool discard;

    /**
     * Set to true to not allow any further matches on levels below this node.
     * If more than one Match matches a node and if any Match is not set as bottom
     * then the database will allow further matches below this node within the fetch query.
     */
    bool bottom;

    /**
     * If set > 0 then the running query will note that a node was matched with this specific match
     * ID.
     * The use of matchId is for matches on the next level to requrie that a specific match was
     * actively matching the parent node.
     * Ids are not unique, many matches can have the same id.
     */
    uint8 id;

    /**
     * If set > 0 then this match requires that at least one match matching the parent node had its
     * matchId value set to this.
     */
    uint8 requireId;

    /**
     * When paging we use this do cutoff everything before and up til this node.
     * The Match is inert until the cursor node is passed.
     * However the Match must match the cursor node to be able to mark it as passed.
     *
     * Note that if the cursor is not found no error is returned, just an empty resultset.
     */
    byte[] cursorId1;
}

/**
 * This struct is used in queries for the client to restrict which nodes are passed along as
 * embedded nodes.
 * Typically a client would defined that is wants Licenses to be returned as embedded.
 */
struct AllowEmbed {
    /** Up to six bytes, fewer bytes means wildcard match on node type. */
    byte[] nodeType;

    /** For a Match to be valid all Filters (if any given) have to match. */
    Filter[] filters;
}

/**
 * Each FetchRequest has one fetch query.
 */
struct FetchQuery {
    /**
     * Max depth of request. Default is -1 meaning max limit. Max allowed depth is 100000 nodes deep.
     */
    int32 depth;

    /**
     * Maximum nr of nodes in total returned. Default is -1 meaning max limit.
     * Max allowed is 1_000_000 nodes (defined by MAX_QUERY_ROWS_LIMIT).
     *
     * Note that this limit includes all data and license nodes, all automatically included
     * license nodes and also all nodes to be embedded.
     *
     * This limit should be used as a maximum upper bound of nodes tolerated in the resultset and not
     * to be used for any dataset limit purposess as the mix of data and licenses nodes will vary
     * depending on how many licenses there are available which will effect how many data nodes fit
     * into the limit of the resultset which can yield confusing results.
     *
     * To use fine tuned limits use Match limit and/or CRDT.
     */
    int32 limit;

    /**
     * The cutoff timestamp to use to discard nodes from the resultset.
     * This is used to limit the amount of data being sent again when no changes has
     * accoured in that resultset.
     * This value is compared to the node's storageTime and updatedTime.
     *
     * When fetching with CRDT this must be set to 0.
     */
    int64 cutoffTime;

    /**
     * The ID1 of the root node which is the basis of this query (level 0).
     *
     * The root node will be checked for access permissions, even if discardRoot is set to true.
     *
     * The root node is not allowed to be licensed, to use hasRightsByAssociation or to be flagged as
     * beginRestrictiveWriterMode.
     *
     * While the root node itself is fetched on its ID1, its regular ID (ID2||ID1) is used as
     * parentId for the subsequent level 1 fetch.
     *
     * Unless discardRoot is set the root node will always be returned in the fetch result (regardless
     * of how cutoffTime is set) for every subsequent trigger fetch made.
     *
     * rootNodeId1 is mutually exclusive to parentId.
     */
    byte[] rootNodeId1;

    /**
     * If set do not return root node in the result set.
     *
     * Only applicable if rootNodeId1 is set.
     */
    bool discardRoot;

    /**
     * The ID of the parent of level 1 fetching.
     *
     * This replaces the need of a root node as the fetching starts directly at level 1 using this propery as parentId.
     *
     * parentId is mutually exclusive to rootNodeId1.
     */
    byte[] parentId;

    /**
     * Optional.
     *
     * For whom this read is performed. Permissions are applied to this public key.
     *
     * Upon arrival this is by default set to the public key of the peer sending the message.
     *
     * It can be set differently by the sender if the receiving P2PClient is configured with
     * allowUncheckedAccess.
     */
    byte[] targetPublicKey;

    /**
     * Optional.
     *
     * Who is the source of the data we are fetching.
     *
     * Upon arrival this is by default set to the public key of the peer receiving this message.
     *
     * It can be set differently by the sender if the receiving P2PClient is configured with
     * allowUncheckedAccess.
     */
    byte[] sourcePublicKey;

    /** At least one Match must match for a node to be added to result set. */
    Match[] match;

    /**
     * The node types and criterias which must match for the Database to send nodes-to-be-embedded
     * back to the client.
     *
     * The client requests for example a License from an Intermediary who is proxying the request to
     * the Storage and also has allowEmbed for License set in its permissions then Licenses will be
     * embedded and subsequently returned in the result set.
     *
     * Note that nodes which get embedded when fetching will not be returned in the first result set,
     * it will take a second fetch or that the first fetch also has a subscription for the embedded
     * nodes to be returned.
     */
    AllowEmbed[] embed;

    /**
     * If set then the query will init a subscription which is run whenever a node is
     * added/changed below the triggerNodeId.
     * triggerNodeId is the parentId of the nodes to observe, it does not need to be related to
     * the parentId or the rootNodeId1 of the query.
     *
     * The Storage is not required to support this feature and if it does not support this feature
     * then a malformed error is returned.
     */
    byte[] triggerNodeId;

    /**
     * Set > 0 to init a subscription to rerun the query on an interval in seconds.
     * Smallest interval is 60 seconds.
     *
     * This can be set regardless if/not triggerNodeId is set and is unsubscribed in the same way.
     *
     * The Storage is not required to support this feature and if it does not support this feature
     * then an error with the status malformed is returned.
     *
     * If using a CRDT together with a subscription triggerNodeId, then the
     * triggerInterval must also be set to properly detect expired nodes.
     *
     * It can be set with triggerNodeId not set just to keep the model updated on regular intervals.
     */
    uint16 triggerInterval;

    /**
     * Set to true to only query on trigger, meaning do not perform an initial fetch,
     * but only fetch on triggerNodeId activation and/or triggerInterval.
     *
     * Cannot be set if fetching with CRDT, because we need the full dataset for the CRDTs.
     */
    bool onlyTrigger;

    /**
     * Nodes per level are by default sorted ascending (oldest first) on creationTime,
     * set this to true to reverse the order.
     * Note that the ordering is only applied per level of the fetch, not for the full fetch
     * and not per parent, but per all children per all parents on the same level.
     *
     * If orderByStorageTime is set then order by node storageTime instead of creationTime.
     */
    bool descending;

    /**
     * If set then order by storageTime instead of by creationTime.
     * This means that ordering is by when the node was first stored in the particular storage
     * instead of when it was created.
     * This will naturally yield different orderings between different storages since the storage
     * timestamp will most often differ.
     * This feature can be useful for presenting nodes in the order they were first seen instead
     * of the creationTime which the creator can set arbitrarely.
     */
    bool orderByStorageTime;

    /**
     * Set to true to not include not valid online nodes in result. Default is to include not valid
     * nodes in resultset.
     */
    bool ignoreInactive;

    /**
     * Set to true to ignore data which has owner targetPublicKey.
     *
     * Default is false.
     */
    bool ignoreOwn;

    /**
     * Set to true to preserve nodes transient values across serialization boundaries when fetching.
     *
     * This is useful when a client wants to piggy-back on the peer's knowledge of the online
     * transient properties of nodes, such as if it is validate and also for CRDT annotations.
     *
     * Only use this if trusting the peer, also transient values are not guaranteed to be provided by
     * the peer.
     */
    bool preserveTransient;

    /**
     * If set then the query will be limited to a certain physical region.
     * A node can be configured for a certain region which must match this setting.
     *
     * The format is ISO 3166-1 "SE", "EU", etc.
     */
    string region;

    /**
     * If set then the query will be limited to a certain jurisdiction.
     * A node can be configured for a certain jurisdiction which must match this setting.
     *
     * The format is ISO 3166-1 "SE", "EU", etc.
     */
    string jurisdiction;

    /**
     * The includeLicenses feature is a way of bundling applicable licenses in the response.
     *
     * Compared to adding a Match for license and an Embed to be able to get new licenses,
     * the includeLicenses feature makes it even more to the point of only targeting licenses
     * needed for the particular nodes matched.
     *
     * Fetching licenses using a Match and Embed sweeps very broadly and might return
     * many licenses for nodes which are of no interest to the query request.
     *
     * If set to 0 this feature is not active.
     *
     * If set to 1 then include all valid existing licenses for each specific node matched,
     * including read and write licenses. Although only read licenses will result in the
     * node being returned in the query response.
     *
     * If set to 2 then automatically add proposed embeddings of licenses which could give
     * rights to nodes matched. This also includes write licenses, although those do not give
     * read access to the nodes.
     *
     * If set to 3 then do for 1 and 2.
     */
    string includeLicenses;
}

/**
 * A fetchRequest can have a FetchCRDT to apply a CRDT the result from the FetchQuery.
 */
struct FetchCRDT {
    /**
     * The ID of the CRDT algo requested.
     * The server might not allow certain or any algos.
     * A value of 0 means CRDT is not used.
     */
    uint16 algo;

    /**
     * Algos can take configuration parameters in JSON format, provided here.
     * Algo sorted and Algo refId both can handle annotations.
     * Annotations is a strategy of further specialising a CRDT algorithm.
     *
     * The below configuration works for the Algo Sorted and Algo RefId, to add
     * annotations to messages for reactions and nested conversations:
     * {
     *  "annotations": {
     *      "format": "messages"
     *  }
     * }
     *
     * Since this is arbitrary JSON data it can also be used to differentiate between
     * different fetch requests to force identical requests to have their own underlying
     * CRDT models, which might be useful in paging scenarios where models need to stay
     * static over some period of time.
     * Set a value like "{differentiator: 1}" to force uniqueness.
     */
    string conf;

    /**
     * Set this property to the msgId of an active subscription request to update its properties.
     *
     * All properties of the fetchRequest must be set the same as the original request,
     * except triggerNodeId which is ignored.
     *
     * The properties of crdt.head, tail, cursorId1, cursorIndex and reverse
     * are allowed to be set differently to update the view of the model, also
     * query.triggerInterval can be set to be updated, if set to 0 then the current value is kept.
     *
     * The response on this request is an empty fetch response, but on the streaming request
     * there will be fresh output.
     */
    byte[] msgId;

    /**
     * Fetch results in reverse order.
     * Reverse model then get view.
     *
     * For head fetches:
     * If the result without reverse is a,b,c,d,e then the result using reverse will be e,d,c,b,a.
     * Furthermore if cursor is "c", without reverse result is d,e and with reverse result is b,a.
     *
     * For tail fetches:
     * If the result without reverse is a,b,c,d,e then the result using reverse will be e,d,c,b,a,
     * furthermore if cursor is "c", without reverse result is a,b and with reverse result is e,d.
     *
     */
    bool reverse;

    /**
     * Read a number if nodes from the start of the sorted model,
     * -1 means until end (or maximum 100000 nodes).
     *
     * This value is mutually exclusive to tail. Set it to 0 if tail is set.
     *
     * This value can be changed for streaming requests to change the scope of the resultset.
     */
    int32 head;

    /**
     * Read a number if nodes from the end of the sorted model,
     * -1 means until start (or maximum 100000 nodes).
     *
     * This value is mutually exclusive to head. Set it to 0 if head is set.
     *
     * This value can be changed for streaming requests to change the scope of the resultset.
     */
    int32 tail;

    /**
     * This can be used to page the results when fetching.
     *
     * The index of the node with the id1 equal to cursorId1 is the previous element in the list
     * and the first returned element is the node after the cursor node.
     *
     * If a cursor is given but not found then the fetch response status is MISSING_CURSOR.
     *
     * This value can be changed for streaming requests to change the scope of the resultset.
     *
     * When fetching using head, the start index will be the index of cursorId1 + 1.
     *
     * When fetching using tail, the end index will be the index of cursorId1 (not including),
     * meaning the window will be moved upwards from the bottom of the list where the cursor
     * is the last node (not included).
     */
    byte[] cursorId1;

    /**
     * This can be used to page the results when fetching.
     *
     * It is the index of the presumed cursor element.
     *
     * This value can be set on its own or as a fallback to cursorId1 in the case the cursor
     * element is not found on its id1.
     *
     * The first element retrieved is cursorOffset + 1, for head fetch.
     * The last element retrived is cursorOffset -1, for tail fetch.
     *
     * Note that a value of -1 indicates this property is not used, while a value of 0
     * means to use element at index 0 as cursor element.
     */
    int32 cursorIndex;
}

/**
 * Each response has a Status property which states the completion status of the request.
 */
enum Status {
    /** This means a result was returned. Which is a success. */
    Result              = 1;

    /** This is returned if the incoming request was malformed and could not be parsed properly. */
    Malformed           = 2;

    /** Some error occoured while processing the request. */
    Error               = 3;

    /** If a storeRequest or writeBlobRequest could not succeed. */
    StoreFailed         = 4;

    /** If a readBlobRequest fails because blob data is not available. */
    FetchFailed         = 5;

    /**
     * If a fetchRequest refers to a real root ID but the root node
     * was not found then this error code is returned.
     */
    MissingRootnode     = 6;

    /**
     * If a fetchRequest refers to a real root node but the root node is either licensed,
     * uses hasRightsByAssociation or is flagged as beginRestrictiveWriterMode then
     * the given root node cannot be used as a root node.
     */
    RootnodeLicensed    = 7;

    /**
     * If access to root node is denied, or
     * when writing/reading a blob whos node we are not allowed to access,
     */
    NotAllowed          = 8;

    /** If content hash does not match expected hash when writing a blob. */
    Mismatch             = 9;

    /** When a blobWrite got finalized or if the blob already did exist when writing. */
    Exists               = 10;

    /**
     * When a CRDT fetch is done using a cursor but the cursor does not exist, this return
     * status is only for requests using CRDTs, and not applicable for query.cursordId1.
     *
     * Note that this is not regarded as an error message and if the request is a trigger subscription
     * then it will not be automatically removed (as when sending any other error replies).
     */
    MissingCursor       = 11;

    /**
     * Sent from the Storage when a trigger has been dropped.
     * Message is sent with seq=0 so that message is cleared out and onCancel()
     * is triggered on the GetResponse object.
     */
    DroppedTrigger      = 12;
}

/**
 *  A FetchResult is the result of a FetchQuery.
 */
struct FetchResult {
    /** array of buffers where each buffer is a serialized node */
    byte[][] nodes;

    /**
     * The Storage can send this array of serialized nodes which are to be signed and sent back to
     * storage for storing.
     *
     * The serialized node is a proposal from the extender of the embedding node to be signed.
     */
    byte[][] embed;

    /**
     * The next cutoff timestamp to use in any subsequent query to reduce the number of duplicate
     * nodes in the result set.
     */
    int64 cutoffTime;
}

struct CRDTResult {
    /**
     * The delta used to patch old model to updated model.
     * All added nodes are passed in the query response and the delta is for patching
     * the order of the nodes.
     * The first byte of the delta tells us what diff algorithm has been used and how is
     * the patch formatted.
     * As for now we support one algo which is FossilDelta and the byte-prefix for that is a
     * single zero byte. The rest of the data is a buffer encoded JSON string, as: {
     *  patch: <fossilDelta patch>
     * }
     */
    byte[] delta;

    /**
     * The index of the last node in the view, or -1 if no nodes are in the view.
     *
     * If using head and reverse == false, then index will always be equal to the
     * index of the last element in the returned.
     *
     * If using head and reverse == true then index will always be equal to the
     * index of the first element in the view.
     *
     * If using tail and reverse == false then index will always be equal to the index of
     * the first element in the returned view.
     *
     * If using tail and reverse == true then index will also always be equal to the index of
     * the first element in the returned view.
     *
     * When paging this returned value of cursorIndex can be fed into the next request,
     * unless it is -1 then no results were returned and paging has reached its end.
     */
    int32 cursorIndex;

    /**
     * The total length of the model.
     * This can be useful when paging to show nr of pages.
     */
    uint32 length;
}

/** The struct used for performing fetch requests. */
@opcode(0x00000001)
struct BopFetchRequest {
    FetchQuery query;
    FetchCRDT crdt;
}

/** The struct used for responding to fetch requests */
@opcode(0x00000002)
struct BopFetchResponse {
    /**
     * Expected status values:
     * Status.RESULT
     * Status.ERROR
     * Status.NOT_ALLOWED
     * Status.ROOTNODE_LICENSED
     * Status.MISSING_ROOTNODE
     * Status.MISSING_CURSOR
     * Status.MALFORMED
     */
    Status status;

    FetchResult result;

    CRDTResult crdtResult;

    /**
     * Counter starting from 1 and will increase for each FetchResponse sent.
     * A batch is identified as having the same endSeq set.
     *
     * If seq == 0 then indicates an error or an unsubscription,
     * and the GetResponse will be cancelled.
     * The message will be removed from the pocket-messaging cache and no more data will flow.
     *
     * A trigger subscription is already removed from the Storage and it is not necessary
     * to explicitly unsubscribe from failed requests.
     */
    uint16 seq;

    /**
     * The seq nr of the last fetchResponse in the batch.
     * If endSeq == 0 then undetermined nr of responses will follow.
     * If endSeq == seq it means this is the last response in this batch, however more batches could
     * follow if the fetch request is a subscription.
     */
    uint16 endSeq;

    /**
     * The number of rows in the database processed so far to return this resultset.
     * If this number is much higher than the expected length of the resulset (or hits the max limit)
     * then it is a good indicator that the query is not specific enough.
     * The rowCount increases for each sequence of the total response returned for a query,
     * meaning it is the aggregated value of all sequences prior for the current query result.
     */
    uint16 rowCount;

    /**
     * If there was an error reported in Status an error message could be provided.
     */
    string error;
}

/** The struct used when sending store requests. */
@opcode(0x00000003)
struct BopStoreRequest {
    /** The serialized data of the nodes to be stored. */
    byte[][] nodes;

    /**
     * Optional.
     *
     * Set who is storing data.
     *
     * Upon arrival this is by default set to the public key of the peer sending the message.
     *
     * It can be set differently by the sender if the receiving P2PClient is configured with
     * allowUncheckedAccess.
     *
     * P2PClientAutoFetchers set it to the public key from where they are fetching data
     * which is then getting stored. It is however reset to its default value upon arrival
     * unless allowUncheckedAccess is set in the receiving P2PClient.
     *
     */
    byte[] sourcePublicKey;

    /**
     * Optional.
     *
     * Set who is the reason we are storing this data.
     *
     * P2PClientExtender set this when extending new licenses and storing them towards
     * the targetPublicKey.
     *
     * Upon arrival this is by default set to the public key of the peer sending the message.
     *
     * It can be set differently by the sender if the receiving P2PClient is configured with
     * allowUncheckedAccess.
     */
    byte[] targetPublicKey;

    /**
     * Could be populated with msg IDs which were the msg IDs of the fetchRequest message sent when
     * creating a subscription.
     * This is useful so that the same data is not bounced back to the peer which the data just was
     * fetched from.
     * Peer clients are not expected to set this.
     */
    byte[][] muteMsgIds;

    /**
     * Set to true to preserve nodes transient values across serialization boundaries when sending
     * nodes to storage.
     * One usage of this is when a client fetches from a peer using fetchQuery.preserveTransient and
     * then wanting to store the transient values in its storage,
     * because the client's storage might not be capable of looking up transient values it self.
     *
     * The Storage must be configured to allow the preservation of transient values for them to be
     * stored.
     */
    bool preserveTransient;

    /**
     * The peer sending the storage request can signal that it will send more data as part of a batch.
     * This can help the receiving peer postpone costly operations such as updating CRDT models.
     * This is an optional field which defaults to 0 meaning no batch in use.
     * batchId should be a unique uint32 number which is not to be reused by the client.
     * muteMsgIds is preserved only for the first request of the batch, then it is ignored.
     */
    uint32 batchId;

    /**
     * When using batchId this field should be set when more data will be sent in a subsequent
     * request.
     *
     * When the last request of the batch is sent this should be false.
     *
     * If batchId is not > 0 then this field is ignored.
     *
     * Default is false.
     */
    bool hasMore;
}

/** Struct used for responding to store requests. */
@opcode(0x00000004)
struct BopStoreResponse {
    /**
     * Expected status values:
     * Status.RESULT
     * Status.STORE_FAILED
     * Status.MALFORMED
     * Status.ERROR
     */
    Status status;

    /**
     * Node ID1s of all nodes which got stored.
     */
    byte[][] storedId1List;

    /**
     * Node ID1s of all nodes in StoreRequest which are missing blobs.
     * Use this to know what blobs to download from the peer.
     */
    byte[][] missingBlobId1List;

    /**
     * Corresponds to missingBlobId1s and gives the size of the blob in bytes.
     */
    int64[] missingBlobSizes;

    /**
     * If there was an error reported in Status an error message could be provided.
     */
    string error;
}

/** Struct used to send unsubscribe requests created on a prior fetch request. */
@opcode(0x00000005)
struct BopUnsubscribeRequest {
    /**
     * The msg ID of the fetch request message sent prior, from which we want to unsubscribe.
     */
    byte[] originalMsgId;

    /**
     * Optional.
     *
     * Who is the fetcher now unsubscribing from a prior fetch request.
     * This public key must match the targetPublicKey in the fetch query we are
     * now unsubscring from.
     *
     * Upon arrival this is by default set to the public key of the peer sending the message.
     *
     * It can be set differently by the sender if the receiving P2PClient is configured with
     * allowUncheckedAccess.
     */
    byte[] targetPublicKey;
}

/** Struct used for responding to unsubscribe requests. */
@opcode(0x00000006)
struct BopUnsubscribeResponse {
    /**
     * Expected status values:
     * Status.RESULT
     */
    Status status;

    /**
     * If there was an error reported in Status an error message could be provided.
     */
    string error;
}

/** Struct used for sending write blob requests. */
@opcode(0x00000007)
struct BopWriteBlobRequest {
    /**
     * The node ID1 of the node we are writing blob data for.
     */
    byte[] nodeId1;

    /**
     * Optional.
     *
     * Who is the sender. Permissions are applied to this public key and access is
     * needed to the blob node to be able to write blob data.
     *
     * Upon arrival this is by default set to the public key of the peer sending the message.
     *
     * It can be set differently by the sender if the receiving P2PClient is configured with
     * allowUncheckedAccess.
     */
    byte[] sourcePublicKey;

    /**
     * Optional.
     *
     * Set for whom we are storing this blob data.
     *
     * Upon arrival this is by default set to the public key of the peer receiving this message.
     *
     * It can be set differently by the sender if the receiving P2PClient is configured with
     * allowUncheckedAccess.
     */
    byte[] targetPublicKey;

    /**
     * The data of the blob to write at the given positon.
     */
    byte[] data;

    /**
     * The position in the blob to write at.
     */
    uint64 pos;

    /**
     * Same as for StoreRequest.
     */
    byte[][] muteMsgIds;
}

/** Struct used for responding to write blob requests. */
@opcode(0x00000008)
struct BopWriteBlobResponse {
    /**
     * Expected status values:
     * Status.ERROR some error occurred
     * Status.NOT_ALLOWED if access to node is not allowed, node not found or if allowWriteBlob is
     * set to false.
     * Status.MALFORMED if input values are wrong or if node is not configured for blob.
     * Status.EXISTS if the blob already exists or if the blob just got finalized to exist from this
     * write or copy action
     * Status.STORE_FAILED if data could not be written or finalized. Database could be busy.
     * Status.MISMATCH if hash does not compute after all data is written.
     * Status.RESULT on successful write, currentLength is set to the size of the written data so far.
     *      The length is the continuous length from start til first gap.
     *      This info can be used for resuming writes.
     */
    Status status;

    /**
     * The current length of the blob data written.
     * If Status.EXISTS is returned currentLength is set to the full length of the blob.
     * If Status.RESULT is returned currentLength is set to the current length of the total
     * continuous blob data written.
     */
    uint64 currentLength;

    /**
     * If there was an error reported in Status an error message could be provided.
     */
    string error;
}

/** Struct used for sending read blob requests. */
@opcode(0x00000009)
struct BopReadBlobRequest {
    /**
     * The node ID1 of the node we are reading blob data for.
     */
    byte[] nodeId1;

    /**
     * Optional.
     *
     * For whom this blob read is performed. Permissions are applied to this public key.
     *
     * Upon arrival this is by default set to the public key of the peer sending the message.
     *
     * It can be set differently by the sender if the receiving P2PClient is configured with
     * allowUncheckedAccess.
     */
    byte[] targetPublicKey;

    /**
     * Optional.
     *
     * Who is the source of the blob data we are fetching.
     *
     * Upon arrival this is by default set to the public key of the peer receiving this message.
     *
     * It can be set differently by the sender if the receiving P2PClient is configured with
     * allowUncheckedAccess.
     */
    byte[] sourcePublicKey;

    /**
     * The position in the blob to read from.
     */
    uint64 pos;

    /**
     * Number of bytes to read.
     * Reads are chopped up in sequences of smaller packages in the KiB range (as is fetching).
     * Length cannot be greater than 1 MiB, if blob is larger than a subsequent request must be made.
     */
    uint32 length;
}

/** Struct used for responding to read blob requests. */
@opcode(0x0000000a)
struct BopReadBlobResponse {
    /**
     * Expected status values:
     * Status.ERROR if some error or exception occurred.
     * Status.NOT_ALLOWED if read permissions to the node is not allowed for the client,target combo
     * or if allowReadBlob is set to false, or if the node is not found.
     * Status.FETCH_FAILED if blob data is not (yet) available.
     * Status.MALFORMED if input values are wrong or if the node is fetched on id2, or if node is not
     * configured for blob.
     * Status.RESULT on successful read.
     */
    Status status;

    /** The read data. */
    byte[] data;

    /**
     * Counter starting from 1 and will increase for each ReadBlobResponse sent for the same ReadBlobRequest.
     * If seq == 0 then indicates an error and the message will be removed.
     */
    uint16 seq;

    /**
     * States how many responses to expect.
     * If == 0 then undetermined nr of responses will follow.
     * If endSeq == seq it means this is the last response in this batch and the message will be removed.
     */
    uint16 endSeq;

    /**
     * When successfully reading blob data this property tells the full length in bytes of the blob.
     */
    uint64 blobLength;

    /**
     * If there was an error reported in Status an error message could be provided.
     */
    string error;
}

/**
* Struct used for sending generic messages to peer
* This generic messaging gives two peers the possibility to exchange messages but which are not
* nodes in the tree.
*/
@opcode(0x0000000b)
struct BopGenericMessageRequest {
    /**
     * Arbitrary string which the peer might understand the meaning of.
     */
    string action;

    /**
     * Optional.
     *
     * Upon arrival this is by default set to the public key of the peer sending the message.
     *
     * It can be set differently by the sender if the receiving P2PClient is configured with
     * allowUncheckedAccess.
     */
    byte[] sourcePublicKey;

    /**
     * This can be JSON or whatever else serialized that the peer can understand.
     * Note that any request sent has a total envelope limit of 64 KiB,
     * meaning that any longer messages have to be split up.
     */
    byte[] data;
}

/** Struct used for responding on generic message requests */
@opcode(0x0000000c)
struct BopGenericMessageResponse {
    /**
     * Expected status values is all dependant on how the peers implement their messaging protocol.
     */
    Status status;

    /** Whatever the peer responded with. */
    byte[] data;

    /**
     * If there was an error reported in Status an error message could be provided.
     */
    string error;
}
